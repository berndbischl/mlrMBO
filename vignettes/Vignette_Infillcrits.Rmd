---
title: "mlrMBO: Infill criteria"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mlrMBO)
```

This vignette gives a short example about the usage of infill criteria in `mlrMBO`. Besides the surrogate model the infill criteria provides an important part to the optimization process in MBO. It is responsible for the propose of new design points at each iteration. For this purpose it often takes the mean and variance estimate of the surrogate model into account.     
Different infill criteria can be used in `mlrMBO`. In addition to the most often used Expected Improvement, there are also Confidence Bound, Augmented Expected Improvement, Expected Quantile Improvement, directed indicator based search, the Mean Response and the use of standard error implemented. But it's also possible to generate custom infill criteria.    
In the first step we show the general setting for the infill criteria in `mlrMBO`. In the second step we give two examples with the expected improvement and the mean response.  
For the general usage of `mlrMBO` look at the vignette *mlrMBO: Quick introduction*. 


##General setting for the infill criterion

In `mlrMBO` the infill criterion is defined within the function `setMBOControlInfill()`. This function is an extension of the `MBOControl` object for further customization concerning the infill criterion. In addition to the type you can also set the kind of optimization or hyperparameters of the infill criterion. The type has to be created with `makeMBOInfillCrit()`.

```{r control, eval = FALSE}
ctrl = makeMBOControl()
ctrl = setMBOControlTermination(ctrl, iters = 5L)
ctrl = setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())
```

For infill criteria that use the variance of the surrogate model you need a learner that support the variance estimate. For the mostly used surrogate models Kriging and Random Forest, the estimate is supported. You have to set `predict.type = "se"` in the learner object to get the estimate. For Random Forest there are also the oportunity to choose between different variance estimators, like the jackknife-after-bootstrap (the default) or the noisy bootstrap. 

```{r rf, eval = FALSE}
lrn.rf = makeLearner("regr.randomForest", predict.type = "se", se.method = "jackknife")
```



## Examples

In the following two examples we want to minimize a simple one dimensional test function with MBO and the mean response as well as the expected improvement. For both examples we take the same test function and general configurations of MBO. 

### Mean Response

The Mean Response as infill criterion proposes new points at the optimum of the estimated mean of the surrogate model. Hence, in case of minimization the target function it takes the minima of the surrogate model and in case of maximization it takes the maxima. For this reason, the focus is on exploitation. It doesn't take the uncertainty of the surrogate model into account. Because of this, it can happen that only a local optima of the target function will be found. We will see that case in the following example. 

In the first step we build a simple test function and generate an inital design:   

```{r fun}
test.fun = makeSingleObjectiveFunction( 
 fn = function(x) x * sin(14 * x),
 par.set = makeNumericParamSet(lower = 0, upper = 1, len = 1L)
)

set.seed(4)
design = generateDesign(4L, getParamSet(test.fun), fun = lhs::maximinLHS)
```


In the next step we create a learner object for the surrogate model. In this example we take Kriging as surrogate model.   
Because the mean as infill criterion doesn't take the variance estimate into account we don't set the `predict.type` argument.   

```{r set}
lrn.km = makeLearner("regr.km")
```

Furthermore the control object for the MBO optimization has to be defined. Within `setMBOControlInfill()` we have to set `crit = makeMBOInfillCritMeanResponse()` to choose the mean response as infill criterion. For the optimization of the infill criterion we take the focussearch that is the default.   

```{r ctrl}
ctrl.mean = makeMBOControl()
ctrl.mean = setMBOControlTermination(ctrl.mean, iters = 5L)
ctrl.mean = setMBOControlInfill(ctrl.mean, crit = makeMBOInfillCritMeanResponse())
```

For the optimization run we take `exampleRun()` instead of `mbo()` in order to visualize it with `plotExampleRun()`. As we only want to look at the first and last iteration, we set `iters = c(1L, 5L)`. 
```{r mean, results = "hide", message = FALSE, warning = FALSE}
run.mean = exampleRun(test.fun, design = design, learner = lrn.km, control = ctrl.mean)
plotExampleRun(run.mean, pause = FALSE, iters = c(1L, 5L))
```

On the figures the solid line shows the test function, the dashed line the estimated mean of the surrogate model, the red points the initial design points, green squares further design points and the blue triangles the proposed points.    
We see that new design points are only proposed at the local optimum. Indeed we are seeing here only five iterations, but the optimization also stagnates with more iterations at the local optimum. This is because new proposes are made always at the minimum of the surrogate model and thus there is no exploration anymore.  
Let's try it next with an infill criterion that do exploitation as well as exploration. 


### Expected Improvement

The Expected Improvement takes the mean and uncertainty of the surrogate model into account. Roughly speaking, at the design points you have the knowledge about the true values of the target function. The uncertainty is very low in this area. So you expect that you will not reach a large improvement if you search further in this part. Areas with large uncertainty are more interesting because there is the chance to get a larger improvement. But additionally it's also worth to consider the estimated mean because areas with low mean are more interesting.
That's why the expected improvement do both exploitation and exploration.  

We take the same test function as at the first example. Within the learner object we have to set `predict.type = "se"`, because Expected Improvement requires the variance estimation. 

```{r lrnkm}
lrn.km = makeLearner("regr.km", predict.type = "se")
```

Now we set `crit = makeMBOInfillCritEI()` within `setMBOControlInfill()`. Like with the mean we take also the focussearch (the default) as optimization. 
```{r ei}
ctrl.ei = makeMBOControl()
ctrl.ei = setMBOControlTermination(ctrl.ei, iters = 5L)
ctrl.ei = setMBOControlInfill(ctrl.ei, crit = makeMBOInfillCritEI())
```

Again we take `exampleRun()` instead of mbo to visualize the results. 
```{r plotei, results = "hide", message = FALSE, warning = FALSE}
run.ei = exampleRun(test.fun, design = design, learner = lrn.km, control = ctrl.ei)
plotExampleRun(run.ei, pause = FALSE)
```

On the figures the uncertainty is displayed by the shaded area. The lower part of a plot shows the expected improvement.   
Now we found the global optimum of the test function in only five iterations.    
This two examples show that exploration can be important to find the global optimum. If the learner supports the variance estimate, expected improvement or confidence bound is a good choice for the infill criterion.  




