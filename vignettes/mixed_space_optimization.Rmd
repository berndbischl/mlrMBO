---
title: "Mixed Space Optimization"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: false
vignette: >
  %\VignetteIndexEntry{Mixed Space Optimization}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE, cache = FALSE}
library(mlrMBO)
set.seed(123)
knitr::opts_chunk$set(cache = TRUE, collapse = FALSE, results = 'hold')
knitr::knit_hooks$set(document = function(x){
  gsub("```\n*```r*\n*", "", x)
})
```

## Purpose

This Vignette is supposed to give you an introduction to use **mlrMBO** for mixed-space optimization, meaning to optimize an objective function with a domain that is not only real-valued but also contains discrete values like *names*.

## Mixed Space Optimization

### Objective Function

We construct an exemplary objective function using `smoof::makeSingleObjetiveFunction()`. 
The `par.set` argument has to be a `ParamSet` object from the **ParamHelpers** package, which provides information about the parameters of the objective function and their constraints for optimization.
The objective function will be 3-dimensional with the inputs `j` and `method`.
`j` is in the interval $[0,2\pi]$
The Parameter `method` is categorical and can be either `"a"` or `"b"`.
In this case we want to minimize the function, so we have to set `minimize = TRUE`.
As the parameters are of different types (e.g. numeric and categorical), the function expects a list instead of a vector as its argument, which is specified by `has.simple.signature = FALSE`.
For further information about he **smoof** package we refer to the [github page](https://github.com/jakobbossek/smoof).

```{r objective_function}
library(mlrMBO)
library(ggplot2)

fun = function(x) {
  j = x$j
  method = x$method
  perf = ifelse(method == "a", sin(j), cos(j))
  return(perf)
}

objfun2 = makeSingleObjectiveFunction(
  name = "mixed_example",
  fn = fun,
  par.set = makeParamSet(
    makeNumericParam("j", lower = 0,upper = 2 * pi),
    makeDiscreteParam("method", values = c("a", "b"))
  ),
  has.simple.signature = FALSE,
  minimize = TRUE
)

# visualize the function
autoplot(objfun2)
```

### Sorrogate Learner

For this kind of parameter space a regression method for the surrogate is necessary that supports *factors*.
To list all *mlr* learners that support factors and uncertainty estimation you can run `listLearners("regr", properties = c("factors", "se"))`.
A popular choice for these scenarios is the *Random Forest*.

```{r}
surr.rf = makeLearner("regr.randomForest", predict.type = "se")
```

### Infill Criterion

Although technically possible the *Expected Imrovement* that we used for the numerical parameter space and the *Kriging* surrogate, the *Confidence Bound* (`makeMBOInfillCritCB()`) or also called statistical upper/lower bound is recommended for *Random Forest* regression.
The reason is, that the *Expected Improvement* is founded on the Gaussian posterior distribution given by the Kriging estimator, which is not given by the Random Forest regression.
For *minimization* the *lower* Confidence Bound is given by $UCB(x) = \hat{\mu}(x) - \lambda \cdot \hat{s}(x)$.
We set $\lambda = 5$.
For the infill criteria optimization we set `opt.focussearch.points = 500`, to increase the speed of the tutorial.

```{r control_object}
control2 = makeMBOControl()
control2 = setMBOControlInfill(
  control = control2,
  crit = makeMBOInfillCritCB(cb.lambda = 5),
  opt.focussearch.points = 500
)
```

### Termination

We want to stop after 10 MBO iterations, meaning 10 function evaluations in this example (not including the initial design):

```{r termination}
control2 = setMBOControlTermination(
  control = control2,
  iters = 10
)
```

### Initial Design

The initial design is set to size of 8:

```{r init_design}
design2 = generateDesign(n = 8, par.set = getParamSet(objfun2))
```

Note that the initial design has to be big enough to cover all discrete values and ideally all combinations of discrete values including integers.
If we had 2 discrete variables one with 5 and one with 3 discrete values the initial design should be at least of size 15.
Usually `generateDesign()` takes care that the points are spread uniformly.

### Optimization

Finally, we start the optimization with `mbo()` with suppressed learner output from *mlr* and we print the result object to obtain the input that lead to the best objective:
```{r mbo_run, eval=TRUE}
# Surpresses output of learners 
mlr::configureMlr(show.info = FALSE, show.learner.output = FALSE, on.learner.warning = "quiet")
run2 = mbo(objfun2, design = design2, learner = surr.rf, control = control2, show.info = TRUE)
print(run2)
```

## Visualization

Visualization is only possible for 2-dimensional objective functions, of which one is the categorical dimension.
Exactly like in our exemplary function.

Again, to obtain all the data that is necessary to generate the plot we have to call the optimization through `exampleRun()`:

```{r example_run_2d}
ex.run2 = exampleRun(objfun2, design = design2, learner = surr.rf, control = control2)
```

And let's visualize the results:

```{r plot_example_run_2d, warning=FALSE}
plotExampleRun(ex.run2, iters = c(1L, 2L, 10L), pause = FALSE)
```
